{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/kanchan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kanchan/Downloads/Courses/DL(DA6401)/CS23S025-Assignment-1_DA6401-DL/wandb/run-20250306_233242-hr5bpnz7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL/runs/hr5bpnz7' target=\"_blank\">genial-forest-8</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL/runs/hr5bpnz7' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL/runs/hr5bpnz7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL/runs/hr5bpnz7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x774e2550a960>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")\n",
    "wandb.init(project=\"CS23S025-Assignment-1-DA6401-DL\", entity=\"cs23s025-indian-institute-of-technology-madras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Fashion-MNIST images are 28x28, so input size is 784. The output is 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# list to hold one sample per class\n",
    "sample_images = [None] * len(categories)\n",
    "\n",
    "for img, lbl in zip(X_train, Y_train):\n",
    "    if sample_images[lbl] is None:\n",
    "        sample_images[lbl] = img  \n",
    "    \n",
    "    if not any(x is None for x in sample_images):\n",
    "        break\n",
    "\n",
    "wandb_images = [wandb.Image(img.astype(np.uint8), caption=categories[idx]) for idx, img in enumerate(sample_images)]\n",
    "wandb.log({\"Q1_sampleImageForEachClass\": wandb_images})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "# for ax, img, lbl in zip(axes.flat, sample_images, categories):\n",
    "#     ax.imshow(img, cmap='gray')\n",
    "#     ax.set_title(lbl)\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to normalise the data.\n",
    "Why to normalise ::\n",
    "1. Faster convergence.\n",
    "2. Avoids vanishing gradients.\n",
    "3. Compatible with activation functions.\n",
    "4. improves Model performance.\n",
    "\n",
    "we did not normalise above because we were just visualising the images but whenever we use the data set for training, normalisation becomes essential for better performance.\n",
    "\n",
    "Why are we dividing with 255.0 for normalisation?\n",
    "-> Fashion-MNIST consists of grayscale images with pixel value ranging from 0 to 255. Dividing by 255 scales all values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions and their corresponding Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def dRelu(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dTanh(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dSigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX / np.sum(expX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # dict<function, derivative>\n",
    "    activationFunc = {\n",
    "        'tanh': (tanh, dTanh),\n",
    "        'sigmoid': (sigmoid, dSigmoid),\n",
    "        'relu': (relu, dRelu),\n",
    "        # Derivative for softmax is handled with cross-entropy loss\n",
    "        'softmax': (softmax, None)  \n",
    "    }\n",
    "    \n",
    "    def __init__(self, inputSize, neuronCount, activation):\n",
    "        # Xavier initialization for weights to help with gradient flow\n",
    "        np.random.seed(33)\n",
    "        sd = np.sqrt(2 / float(inputSize + neuronCount))\n",
    "        self.w = np.random.normal(0, sd, size=(neuronCount, inputSize))\n",
    "        self.b = np.zeros((neuronCount, 1))\n",
    "        self.act, self.dAct = self.activationFunc.get(activation)\n",
    "        \n",
    "        # Placeholders for forward pass outputs\n",
    "        self.a = None  # Pre-activation \n",
    "        self.h = None  # Activation \n",
    "        \n",
    "        # Placeholders for gradients computed during backpropagation\n",
    "        self.da = None  # Gradient w.r.t. pre-activation\n",
    "        self.dW = None  # Gradient for weights\n",
    "        self.db = None  # Gradient for bias\n",
    "        self.dh = None  # Gradient for activation from the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(inputData, layers):\n",
    "    \"\"\"\n",
    "      inputData: numpy array of shape (inputDim, 1)\n",
    "      layers: list of Layer objects ordered from the first hidden to the output layer\n",
    "      Returns:Output probabilities from the final layer\n",
    "    \"\"\"\n",
    "    numLayers = len(layers)\n",
    "    # First layer\n",
    "    layers[0].a = np.dot(layers[0].w, inputData) + layers[0].b\n",
    "    layers[0].h = layers[0].act(layers[0].a)\n",
    "    \n",
    "    #hidden layers\n",
    "    for j in range(1, numLayers - 1):\n",
    "        layers[j].a = np.dot(layers[j].w, layers[j-1].h) + layers[j].b\n",
    "        layers[j].h = layers[j].act(layers[j].a)\n",
    "    \n",
    "    # Output layer\n",
    "    layers[numLayers - 1].a = np.dot(layers[numLayers - 1].w, layers[numLayers - 2].h) + layers[numLayers - 1].b\n",
    "    layers[numLayers - 1].h = softmax(layers[numLayers - 1].a)\n",
    "    \n",
    "    return layers[numLayers - 1].h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(trueLabel, yHat, layers, inputData):\n",
    "    \"\"\"\n",
    "Used to compute gradients for each layer.\n",
    "\n",
    "trueLabel: the true class index (integer)\n",
    "yHat: predicted output from forwardPropagation (probabilities)\n",
    "layers: list of Layer objects\n",
    "inputData: original input data (needed for first layer gradient)\n",
    "  \n",
    "  returns : The layers list with updated gradients (dW and db for each layer).\n",
    "    \"\"\"\n",
    "    # one-hot encoded vector for the true label\n",
    "    oneHot = np.zeros(yHat.shape)\n",
    "    oneHot[trueLabel] = 1\n",
    "    \n",
    "    # For output layer using softmax and cross-entropy loss:\n",
    "    layers[-1].da = yHat - oneHot  # (yHat - oneHot) is the gradient\n",
    "    \n",
    "    # Backpropagate from the output layer to the first hidden layer\n",
    "    for j in range(len(layers) - 1, 0, -1):\n",
    "\n",
    "        # gradients for weights and biases for the current layer\n",
    "        prevActivation = layers[j-1].h  # Activation from previous layer\n",
    "        layers[j].dW = np.dot(layers[j].da, prevActivation.T)\n",
    "        layers[j].db = layers[j].da \n",
    "        \n",
    "        # gradient for previous layer's activation\n",
    "        layers[j-1].dh = np.dot(layers[j].w.T, layers[j].da)\n",
    "        if layers[j-1].dAct is not None:\n",
    "            layers[j-1].da = layers[j-1].dh * layers[j-1].dAct(layers[j-1].a)\n",
    "    \n",
    "    # gradients for the first layer using the original in data\n",
    "    layers[0].dW = np.dot(layers[0].da, inputData.T)\n",
    "    layers[0].db = layers[0].da\n",
    "    \n",
    "    return layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
