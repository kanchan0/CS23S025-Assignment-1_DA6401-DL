{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")\n",
    "wandb.init(project=\"CS23S025-Assignment-1-DA6401-DL\", entity=\"cs23s025-indian-institute-of-technology-madras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Fashion-MNIST images are 28x28, so input size is 784. The output is 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# list to hold one sample per class\n",
    "sample_images = [None] * len(categories)\n",
    "\n",
    "for img, lbl in zip(X_train, Y_train):\n",
    "    if sample_images[lbl] is None:\n",
    "        sample_images[lbl] = img  \n",
    "    \n",
    "    if not any(x is None for x in sample_images):\n",
    "        break\n",
    "\n",
    "wandb_images = [wandb.Image(img.astype(np.uint8), caption=categories[idx]) for idx, img in enumerate(sample_images)]\n",
    "wandb.log({\"Q1_sampleImageForEachClass\": wandb_images})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "# for ax, img, lbl in zip(axes.flat, sample_images, categories):\n",
    "#     ax.imshow(img, cmap='gray')\n",
    "#     ax.set_title(lbl)\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to normalise the data.\n",
    "Why to normalise ::\n",
    "1. Faster convergence.\n",
    "2. Avoids vanishing gradients.\n",
    "3. Compatible with activation functions.\n",
    "4. improves Model performance.\n",
    "\n",
    "we did not normalise above because we were just visualising the images but whenever we use the data set for training, normalisation becomes essential for better performance.\n",
    "\n",
    "Why are we dividing with 255.0 for normalisation?\n",
    "-> Fashion-MNIST consists of grayscale images with pixel value ranging from 0 to 255. Dividing by 255 scales all values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions and their corresponding Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def dRelu(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dTanh(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dSigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX / np.sum(expX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # dict<function, derivative>\n",
    "    activationFunc = {\n",
    "        'tanh': (tanh, dTanh),\n",
    "        'sigmoid': (sigmoid, dSigmoid),\n",
    "        'relu': (relu, dRelu),\n",
    "        # Derivative for softmax is handled with cross-entropy loss\n",
    "        'softmax': (softmax, None)  \n",
    "    }\n",
    "    \n",
    "    def __init__(self, inputSize, neuronCount, activation):\n",
    "        # Xavier initialization for weights to help with gradient flow\n",
    "        np.random.seed(33)\n",
    "        sd = np.sqrt(2 / float(inputSize + neuronCount))\n",
    "        self.w = np.random.normal(0, sd, size=(neuronCount, inputSize))\n",
    "        self.b = np.zeros((neuronCount, 1))\n",
    "        self.act, self.dAct = self.activationFunc.get(activation)\n",
    "        \n",
    "        # Placeholders for forward pass outputs\n",
    "        self.a = None  # Pre-activation \n",
    "        self.h = None  # Activation \n",
    "        \n",
    "        # Placeholders for gradients computed during backpropagation\n",
    "        self.da = None  # Gradient w.r.t. pre-activation\n",
    "        self.dW = None  # Gradient for weights\n",
    "        self.db = None  # Gradient for bias\n",
    "        self.dh = None  # Gradient for activation from the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(inputData, layers):\n",
    "    \"\"\"\n",
    "      inputData: numpy array of shape (inputDim, 1)\n",
    "      layers: list of Layer objects ordered from the first hidden to the output layer\n",
    "      Returns:Output probabilities from the final layer\n",
    "    \"\"\"\n",
    "    numLayers = len(layers)\n",
    "    # First layer\n",
    "    layers[0].a = np.dot(layers[0].w, inputData) + layers[0].b\n",
    "    layers[0].h = layers[0].act(layers[0].a)\n",
    "    \n",
    "    #hidden layers\n",
    "    for j in range(1, numLayers - 1):\n",
    "        layers[j].a = np.dot(layers[j].w, layers[j-1].h) + layers[j].b\n",
    "        layers[j].h = layers[j].act(layers[j].a)\n",
    "    \n",
    "    # Output layer\n",
    "    layers[numLayers - 1].a = np.dot(layers[numLayers - 1].w, layers[numLayers - 2].h) + layers[numLayers - 1].b\n",
    "    layers[numLayers - 1].h = softmax(layers[numLayers - 1].a)\n",
    "    \n",
    "    return layers[numLayers - 1].h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(trueLabel, yHat, layers, inputData):\n",
    "    \"\"\"\n",
    "Used to compute gradients for each layer.\n",
    "\n",
    "trueLabel: the true class index (integer)\n",
    "yHat: predicted output from forwardPropagation (probabilities)\n",
    "layers: list of Layer objects\n",
    "inputData: original input data (needed for first layer gradient)\n",
    "  \n",
    "  returns : The layers list with updated gradients (dW and db for each layer).\n",
    "    \"\"\"\n",
    "    # one-hot encoded vector for the true label\n",
    "    oneHot = np.zeros(yHat.shape)\n",
    "    oneHot[trueLabel] = 1\n",
    "    \n",
    "    # For output layer using softmax and cross-entropy loss:\n",
    "    layers[-1].da = yHat - oneHot  # (yHat - oneHot) is the gradient\n",
    "    \n",
    "    # Backpropagate from the output layer to the first hidden layer\n",
    "    for j in range(len(layers) - 1, 0, -1):\n",
    "\n",
    "        # gradients for weights and biases for the current layer\n",
    "        prevActivation = layers[j-1].h  # Activation from previous layer\n",
    "        layers[j].dW = np.dot(layers[j].da, prevActivation.T)\n",
    "        layers[j].db = layers[j].da \n",
    "        \n",
    "        # gradient for previous layer's activation\n",
    "        layers[j-1].dh = np.dot(layers[j].w.T, layers[j].da)\n",
    "        if layers[j-1].dAct is not None:\n",
    "            layers[j-1].da = layers[j-1].dh * layers[j-1].dAct(layers[j-1].a)\n",
    "    \n",
    "    # gradients for the first layer using the original in data\n",
    "    layers[0].dW = np.dot(layers[0].da, inputData.T)\n",
    "    layers[0].db = layers[0].da\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losss Functions - cross Entropy and squared Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaredErrorLoss(labels, predictions, index):\n",
    "    \n",
    "    numClasses = predictions.shape[0]\n",
    "    # one-hot encoded vector for the true label.\n",
    "    oneHot = np.zeros((numClasses, 1))\n",
    "    oneHot[labels[index]] = 1\n",
    "\n",
    "    # squared error loss b/w predictions and one hot vector.\n",
    "    loss = np.sum((predictions - oneHot) ** 2)\n",
    "    return loss\n",
    "\n",
    "def crossEntropyLoss(labels, predictions, index):\n",
    " \n",
    "    '''\n",
    "        * labels[index] is the true class index, predictions[labels[index]] is \n",
    "        the probability for that class.\n",
    "    '''\n",
    "    return -np.log(predictions[labels[index]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD) training with mini-batch updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize):\n",
    "    \"\"\"\n",
    "    returns the following : \n",
    "      *costHistory -> List of average training cost per epoch.\n",
    "      * layers    ->  Updated network layers with trained weights.\n",
    "    \"\"\"\n",
    "    numSamples = trainX.shape[0]\n",
    "    costHistory = []\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        epochCost = 0\n",
    "        \n",
    "        for i in range(numSamples):\n",
    "           \n",
    "            inputSample = trainX[i].reshape(784, 1)\n",
    "            \n",
    "            # Forward propagation: calc the output prob.\n",
    "            outputProb = forwardPropagation(inputSample, layers)\n",
    "            \n",
    "            # get training loss for the sample using cross-entropy loss.\n",
    "            epochCost += crossEntropyLoss(trainY, outputProb, i)\n",
    "            \n",
    "            # Backward propagation: calc grad based on the true label.\n",
    "            backwardPropagation(trainY[i], outputProb, layers, inputSample)\n",
    "            \n",
    "            # Update parameters when the mini-batch is complete.\n",
    "            if (i + 1) % batchSize == 0:\n",
    "                for layer in layers:\n",
    "                    # Update weights and biases using averaged gradients.\n",
    "                    layer.w = layer.w - learningRate * (layer.dW / batchSize)\n",
    "                    layer.b = layer.b - learningRate * (layer.db / batchSize)\n",
    "                    \n",
    "                    # gradients is resetted to zero for the next mini-batch.\n",
    "                    layer.dW = 0\n",
    "                    layer.db = 0\n",
    "        \n",
    "        # avg cost for the current epoch.\n",
    "        costHistory.append(epochCost / numSamples)\n",
    "        \n",
    "        # Evaluating the Model\n",
    "        # Entire validation set is passed as a batch\n",
    "        valPrediction = forwardPropagation(valX.T, layers)\n",
    "        valCost = 0\n",
    "        for i in range(len(valY)):\n",
    "            valCost += crossEntropyLoss(valY, valPrediction[:, i].reshape(10, 1), i)\n",
    "        valCost /= len(valY)\n",
    "        \n",
    "        # validation accuracy\n",
    "        predictedLabels = valPrediction.argmax(axis=0)\n",
    "        valAccuracy = np.sum(predictedLabels == valY) / valY.shape[0]\n",
    "        \n",
    "        # wandb logs\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": costHistory[-1], \"val_accuracy\": valAccuracy, \"val_loss\": valCost})\n",
    "        \n",
    "        print(f\"-----------------Epoch {epoch}-----------------\")\n",
    "        print(\"Training Loss: \", epochCost / numSamples)\n",
    "        print(\"Validation Accuracy: \", valAccuracy)\n",
    "        print(\"Validation Loss: \", valCost)\n",
    "    \n",
    "    return costHistory, layers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
