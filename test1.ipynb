{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")\n",
    "wandb.init(project=\"CS23S025-Assignment-1-DA6401-DL\", entity=\"cs23s025-indian-institute-of-technology-madras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Fashion-MNIST images are 28x28, so input size is 784. The output is 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# list to hold one sample per class\n",
    "sample_images = [None] * len(categories)\n",
    "\n",
    "for img, lbl in zip(X_train, Y_train):\n",
    "    if sample_images[lbl] is None:\n",
    "        sample_images[lbl] = img  \n",
    "    \n",
    "    if not any(x is None for x in sample_images):\n",
    "        break\n",
    "\n",
    "wandb_images = [wandb.Image(img.astype(np.uint8), caption=categories[idx]) for idx, img in enumerate(sample_images)]\n",
    "wandb.log({\"Q1_sampleImageForEachClass\": wandb_images})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "# for ax, img, lbl in zip(axes.flat, sample_images, categories):\n",
    "#     ax.imshow(img, cmap='gray')\n",
    "#     ax.set_title(lbl)\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to normalise the data.\n",
    "Why to normalise ::\n",
    "1. Faster convergence.\n",
    "2. Avoids vanishing gradients.\n",
    "3. Compatible with activation functions.\n",
    "4. improves Model performance.\n",
    "\n",
    "we did not normalise above because we were just visualising the images but whenever we use the data set for training, normalisation becomes essential for better performance.\n",
    "\n",
    "Why are we dividing with 255.0 for normalisation?\n",
    "-> Fashion-MNIST consists of grayscale images with pixel value ranging from 0 to 255. Dividing by 255 scales all values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping\n",
    "xTrainTemp = X_train.reshape(X_train.shape[0], -1)  # (60000, 784)\n",
    "yTrainTemp = Y_train\n",
    "xTestTemp = X_test.reshape(X_test.shape[0], -1)       # (10000, 784)\n",
    "yTestTemp = Y_test\n",
    "\n",
    "print(\"xTrainTemp shape:\", xTrainTemp.shape)\n",
    "print(\"yTrainTemp shape:\", yTrainTemp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets (90/10 split)\n",
    "x_train, x_val, y_train, y_val = train_test_split(xTrainTemp, yTrainTemp, test_size=0.1, random_state=33)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions and their corresponding Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def dRelu(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dTanh(x):\n",
    "    return 1 - np.square(np.tanh(x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dSigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX / np.sum(expX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # dict<function, derivative>\n",
    "    activationFunc = {\n",
    "        'tanh': (tanh, dTanh),\n",
    "        'sigmoid': (sigmoid, dSigmoid),\n",
    "        'relu': (relu, dRelu),\n",
    "        # Derivative for softmax is handled with cross-entropy loss\n",
    "        'softmax': (softmax, None)  \n",
    "    }\n",
    "    \n",
    "    def __init__(self, inputSize, neuronCount, activation):\n",
    "        # Xavier initialization for weights to help with gradient flow\n",
    "        np.random.seed(33)\n",
    "        sd = np.sqrt(2 / float(inputSize + neuronCount))\n",
    "        self.w = np.random.normal(0, sd, size=(neuronCount, inputSize))\n",
    "        self.b = np.zeros((neuronCount, 1))\n",
    "        self.act, self.dAct = self.activationFunc.get(activation)\n",
    "        self.dW = 0 #gradients of the loss function with respect to the weights and biases of the layer, respectively\n",
    "        self.db = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(inputData, layers):\n",
    "    \"\"\"\n",
    "      inputData: numpy array of shape (inputDim, 1)\n",
    "      layers: list of Layer objects ordered from the first hidden to the output layer\n",
    "      Returns:Output probabilities from the final layer\n",
    "    \"\"\"\n",
    "    numLayers = len(layers)\n",
    "    # First layer\n",
    "    layers[0].a = np.dot(layers[0].w, inputData)\n",
    "    layers[0].h = layers[0].act(layers[0].a)\n",
    "    \n",
    "    #hidden layers\n",
    "    for j in range(1, numLayers - 1):\n",
    "        layers[j].a = np.dot(layers[j].w, layers[j-1].h)\n",
    "        layers[j].h = layers[j].act(layers[j].a)\n",
    "    j+=1\n",
    "    # Output layer\n",
    "    layers[j].a = np.dot(layers[j].w, layers[j-1].h) #last layers pre-activation\n",
    "    layers[j].h = softmax(layers[j].a) #output layer activation using softmax fucntion ---> returns probability\n",
    "    return layers[numLayers-1].h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(trueLabel, yHat, layers, inputData):\n",
    "    \"\"\"\n",
    "Used to compute gradients for each layer.\n",
    "\n",
    "trueLabel: the true class index (integer)\n",
    "yHat: predicted output from forwardPropagation (probabilities)\n",
    "layers: list of Layer objects\n",
    "inputData: original input data (needed for first layer gradient)\n",
    "  \n",
    "  returns : The layers list with updated gradients (dW and db for each layer).\n",
    "    \"\"\"\n",
    "    # one-hot encoded vector for the true label\n",
    "    oneHot = np.zeros(yHat.shape)\n",
    "    oneHot[trueLabel] = 1\n",
    "    \n",
    "    # For output layer using softmax and cross-entropy loss:\n",
    "    layers[-1].da = yHat - oneHot  # (yHat - oneHot) is the gradient\n",
    "    \n",
    "    # Backpropagate from the output layer to the first hidden layer\n",
    "    for j in range(len(layers) - 1, 0, -1):\n",
    "\n",
    "        # gradients for weights and biases for the current layer\n",
    "        prevActivation = layers[j-1].h  # Activation from previous layer\n",
    "        layers[j].dW = np.dot(layers[j].da, prevActivation.T)\n",
    "        layers[j].db = layers[j].da \n",
    "        \n",
    "        # gradient for previous layer's activation\n",
    "        layers[j-1].dh = np.dot(layers[j].w.T, layers[j].da)\n",
    "        if layers[j-1].dAct is not None:\n",
    "            layers[j-1].da = layers[j-1].dh * layers[j-1].dAct(layers[j-1].a)\n",
    "    \n",
    "    # gradients for the first layer using the original ip data\n",
    "    layers[0].dW = np.dot(layers[0].da, inputData.T)\n",
    "    layers[0].db = layers[0].da\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losss Functions - cross Entropy and squared Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaredErrorLoss(labels, predictions, index):\n",
    "    \n",
    "    numClasses = predictions.shape[0]\n",
    "    # one-hot encoded vector for the true label.\n",
    "    oneHot = np.zeros((numClasses, 1))\n",
    "    oneHot[labels[index]] = 1\n",
    "\n",
    "    # squared error loss b/w predictions and one hot vector.\n",
    "    loss = np.sum((predictions - oneHot) ** 2)\n",
    "    return loss\n",
    "\n",
    "def crossEntropyLoss(labels, predictions, index):\n",
    " \n",
    "    '''\n",
    "        * labels[index] is the true class index, predictions[labels[index]] is \n",
    "        the probability for that class.\n",
    "    '''\n",
    "    return -np.log(predictions[labels[index]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD) training with mini-batch updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize):\n",
    "    \"\"\"\n",
    "    returns the following : \n",
    "      *costHistory -> List of average training cost per epoch.\n",
    "      * layers    ->  Updated network layers with trained weights.\n",
    "    \"\"\"\n",
    "    numSamples = trainX.shape[0]\n",
    "    costHistory = []\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        epochCost = 0\n",
    "        \n",
    "        for i in range(numSamples):\n",
    "           \n",
    "            inputSample = trainX[i].reshape(784, 1)\n",
    "            print(inputSample[0].shape) \n",
    "            # Forward propagation: calc the output prob.\n",
    "            outputProb = forwardPropagation(inputSample, layers)\n",
    "            \n",
    "            # get training loss for the sample using cross-entropy loss.\n",
    "            epochCost += crossEntropyLoss(trainY, outputProb, i)\n",
    "            \n",
    "            # Backward propagation: calc grad based on the true label.\n",
    "            backwardPropagation(trainY[i], outputProb, layers, inputSample)\n",
    "            \n",
    "            # Update parameters when the mini-batch is complete.\n",
    "            if (i + 1) % batchSize == 0:\n",
    "                for layer in layers:\n",
    "                    # Update weights and biases using averaged gradients.\n",
    "                    layer.w = layer.w - learningRate * (layer.dW / batchSize)\n",
    "                    layer.b = layer.b - learningRate * (layer.db / batchSize)\n",
    "                    \n",
    "                    # gradients is resetted to zero for the next mini-batch.\n",
    "                    layer.dW = 0\n",
    "                    layer.db = 0\n",
    "        \n",
    "        # avg cost for the current epoch.\n",
    "        costHistory.append(epochCost / numSamples)\n",
    "        \n",
    "        # Evaluating the Model\n",
    "        # Entire validation set is passed as a batch\n",
    "        valPrediction = forwardPropagation(valX.T, layers)\n",
    "        valCost = 0\n",
    "        for i in range(len(valY)):\n",
    "            valCost += crossEntropyLoss(valY, valPrediction[:, i].reshape(10, 1), i)\n",
    "        valCost /= len(valY)\n",
    "        \n",
    "        # validation accuracy\n",
    "        predictedLabels = valPrediction.argmax(axis=0)\n",
    "        valAccuracy = np.sum(predictedLabels == valY) / valY.shape[0]\n",
    "        \n",
    "        # wandb logs\n",
    "        #wandb.log({\"epoch\": epoch, \"train_loss\": costHistory[-1], \"val_accuracy\": valAccuracy, \"val_loss\": valCost})\n",
    "        \n",
    "        print(f\"-----------------Epoch {epoch}-----------------\")\n",
    "        print(\"Training Loss: \", epochCost / numSamples)\n",
    "        print(\"Validation Accuracy: \", valAccuracy)\n",
    "        print(\"Validation Loss: \", valCost)\n",
    "    \n",
    "    return costHistory, layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### momentum based gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentumGradientDescent(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize):\n",
    "    # Momentum hyperparameter\n",
    "    gamma = 0.9  \n",
    "    numSamples = trainX.shape[0]\n",
    "    costHistory = []\n",
    "    \n",
    "    # momentum for each layer once at the start\n",
    "    for layer in layers:\n",
    "        layer.updateW = 0  # Momentum term for weights\n",
    "        layer.updateB = 0  # Momentum term for biases\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        epochCost = 0\n",
    "        \n",
    "        for i in range(numSamples):\n",
    "            # Reshape current training sample into column vector-> 784 x 1 for MNIST\n",
    "            inputSample = trainX[i].reshape(784, 1)\n",
    "            \n",
    "            # Forward propagation: cacl output probab\n",
    "            outputProb = forwardPropagation(inputSample, layers)\n",
    "            \n",
    "            # Calc training loss using cross-entropy\n",
    "            epochCost += crossEntropyLoss(trainY, outputProb, i)\n",
    "            \n",
    "            # Backpropagation: calc grad for current sample\n",
    "            backwardPropagation(trainY[i], outputProb, layers, inputSample)\n",
    "            \n",
    "            # update parameters with momentum when a mini batch is complete\n",
    "            if (i + 1) % batchSize == 0:\n",
    "                for layer in layers:\n",
    "                    # Update velocity\n",
    "                    layer.updateW = gamma * layer.updateW + learningRate * (layer.dW / batchSize)\n",
    "                    layer.updateB = gamma * layer.updateB + learningRate * (layer.db / batchSize)\n",
    "                    \n",
    "                    # Update weights and biases using the momentum terms\n",
    "                    layer.w = layer.w - layer.updateW\n",
    "                    layer.b = layer.b - layer.updateB\n",
    "                    \n",
    "                    # Reset gradients for the next mini-batch\n",
    "                    layer.dW = 0\n",
    "                    layer.db = 0\n",
    "\n",
    "        # Avg training cost for the epoch\n",
    "        costHistory.append(epochCost / numSamples)\n",
    "        \n",
    "        # Evaluating the Model\n",
    "        # Here, we assume the validation data is passed in a batch\n",
    "        valPrediction = forwardPropagation(valX.T, layers)\n",
    "        valCost = 0\n",
    "        for i in range(len(valY)):\n",
    "            valCost += crossEntropyLoss(valY, valPrediction[:, i].reshape(10, 1), i)\n",
    "            \n",
    "        valCost /= len(valY)\n",
    "        predictedLabels = valPrediction.argmax(axis=0)\n",
    "        valAccuracy = np.sum(predictedLabels == valY) / valY.shape[0]\n",
    "        \n",
    "        # wandb logs\n",
    "        #wandb.log({\"epoch\": epoch, \"train_loss\": costHistory[-1], \"val_accuracy\": valAccuracy, \"val_loss\": valCost})\n",
    "        \n",
    "        print(f\"-----------------Epoch {epoch}-----------------\")\n",
    "        print(\"Training Loss: \", epochCost / numSamples)\n",
    "        print(\"Validation Accuracy: \", valAccuracy)\n",
    "        print(\"Validation Loss: \", valCost)\n",
    "    \n",
    "    return costHistory, layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize):\n",
    "    \n",
    "    epsilon, beta1, beta2 = 1e-8, 0.9, 0.99\n",
    "    # Time step for bias correction\n",
    "    t = 0  \n",
    "    numSamples = trainX.shape[0]\n",
    "    costHistory = []\n",
    "    \n",
    "    # Initialize m,v for each layer once at the start.\n",
    "    for layer in layers:\n",
    "        layer.mW = 0  # 1st moment for weights\n",
    "        layer.mB = 0  # 1stmoment for biases\n",
    "        layer.vW = 0  # 2nd  moment for weights\n",
    "        layer.vB = 0  # 2nd  moment for biases\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        epochCost = 0\n",
    "\n",
    "        for i in range(numSamples):\n",
    "            # Reshape current training sample into column vector-> 784 x 1 for MNIST\n",
    "            inputSample = trainX[i].reshape(784, 1)\n",
    "            \n",
    "            # Forward propagation: calc the output probab\n",
    "            outputProb = forwardPropagation(inputSample, layers)\n",
    "            \n",
    "            # Calculate training loss\n",
    "            epochCost += crossEntropyLoss(trainY, outputProb, i)\n",
    "            \n",
    "            # Backpropagation: calc grad for the current sample\n",
    "            backwardPropagation(trainY[i], outputProb, layers, inputSample)\n",
    "            \n",
    "            # Update parameters after processing a mini-batch\n",
    "            if (i + 1) % batchSize == 0:\n",
    "                t += 1  \n",
    "                for layer in layers:\n",
    "                    # Compute mini-batch avg grad\n",
    "                    gradW = layer.dW / batchSize\n",
    "                    gradB = layer.db / batchSize\n",
    "                    \n",
    "                    # Update biased 1st moment estimate\n",
    "                    layer.mW = beta1 * layer.mW + (1 - beta1) * gradW\n",
    "                    layer.mB = beta1 * layer.mB + (1 - beta1) * gradB\n",
    "                    \n",
    "                    # Update biased 2nd moment estimate\n",
    "                    layer.vW = beta2 * layer.vW + (1 - beta2) * (gradW ** 2)\n",
    "                    layer.vB = beta2 * layer.vB + (1 - beta2) * (gradB ** 2)\n",
    "                    \n",
    "                    # Compute bias-corrected 1st moment estimate\n",
    "                    mWHat = layer.mW / (1 - np.power(beta1, t))\n",
    "                    mBHat = layer.mB / (1 - np.power(beta1, t))\n",
    "                    \n",
    "                    # Compute bias-corrected 2nd moment estimate\n",
    "                    vWHat = layer.vW / (1 - np.power(beta2, t))\n",
    "                    vBHat = layer.vB / (1 - np.power(beta2, t))\n",
    "                    \n",
    "                    # Update parameters using the Adam update rule\n",
    "                    layer.w = layer.w - learningRate * mWHat / (np.sqrt(vWHat) + epsilon)\n",
    "                    layer.b = layer.b - learningRate * mBHat / (np.sqrt(vBHat) + epsilon)\n",
    "                    \n",
    "                    # Reset gradients for the next mini-batch\n",
    "                    layer.dW = 0\n",
    "                    layer.db = 0\n",
    "        \n",
    "        # average training cost for the current epoch\n",
    "        costHistory.append(epochCost / numSamples)\n",
    "        \n",
    "        # Evaluation on validation data \n",
    "        valPrediction = forwardPropagation(valX.T, layers)\n",
    "        valCost = 0\n",
    "        for i in range(len(valY)):\n",
    "            valCost += crossEntropyLoss(valY, valPrediction[:, i].reshape(10, 1), i)\n",
    "        valCost /= len(valY)\n",
    "        \n",
    "        # find predicted labels and compute validation accuracy\n",
    "        predictedLabels = valPrediction.argmax(axis=0)\n",
    "        valAccuracy = np.sum(predictedLabels == valY) / valY.shape[0]\n",
    "        \n",
    "        # wandb logs\n",
    "        # wandb.log({\"epoch\": epoch, \"train_loss\": costHistory[-1], \"val_accuracy\": valAccuracy, \"val_loss\": valCost})\n",
    "        \n",
    "        print(f\"-----------------Epoch {epoch}-----------------\")\n",
    "        print(\"Training Loss: \", epochCost / numSamples)\n",
    "        print(\"Validation Accuracy: \", valAccuracy)\n",
    "        print(\"Validation Loss: \", valCost)\n",
    "    \n",
    "    return costHistory, layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myOptimiser(layers, optimizerName, numEpochs, learningRate, trainX, trainY, valX, valY, batchSize):\n",
    "\n",
    "    if optimizerName == \"sgd\":\n",
    "        return sgd(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize)\n",
    "    elif optimizerName == \"mgd\":\n",
    "        return momentumGradientDescent(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize)\n",
    "    # elif optimizerName == \"nesterov\":\n",
    "    #     return nesterov(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize)\n",
    "    # elif optimizerName == \"rmsprop\":\n",
    "    #     return rmsprop(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize)\n",
    "    elif optimizerName == \"adam\":\n",
    "         return adam(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize)\n",
    "    # elif optimizerName == \"nadam\":\n",
    "    #     return nadam(numEpochs, layers, learningRate, trainX, trainY, valX, valY, batchSize)\n",
    "    else:\n",
    "        print(\"No optimization algorithm named \" + optimizerName + \" found\")\n",
    "        return \"Error\", \"Error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputData, trueLabels, layers):\n",
    "\n",
    "    # Forward pass: compute predictions for the entire test data.\n",
    "    predictionMatrix = forwardPropagation(inputData, layers)\n",
    "    \n",
    "    totalLoss = 0\n",
    "    numSamples = len(trueLabels)\n",
    "    for i in range(numSamples):\n",
    "        # Compute cross-entropy loss for each sample.\n",
    "        totalLoss += crossEntropyLoss(trueLabels, predictionMatrix[:, i].reshape(10, 1), i)\n",
    "    \n",
    "    # Determine predicted labels by taking the class with maximum probability.\n",
    "    predictedLabels = predictionMatrix.argmax(axis=0)\n",
    "    accuracy = np.sum(predictedLabels == trueLabels) / trueLabels.shape[0]\n",
    "    \n",
    "    averageLoss = totalLoss / numSamples\n",
    "    return predictedLabels, accuracy, averageLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTrain(epochs, learningRate, neurons, hLayers, activation, batchSize, optimizer, x_train, y_train, x_val, y_val):\n",
    "   \n",
    "    layers = [Layer(x_train.shape[1], neurons, activation)]\n",
    "    for _ in range(hLayers - 1):\n",
    "        layers.append(Layer(neurons, neurons, activation))\n",
    "    layers.append(Layer(neurons, 10, 'softmax'))\n",
    "    \n",
    "    costs, layers = myOptimiser(layers, optimizer, epochs, learningRate, x_train, y_train, x_val, y_val, batchSize)\n",
    "    \n",
    "    # Evaluate the model on the test set.\n",
    "    outputTest, accuracyTest, testLoss = predict(xTestTemp.T, yTestTemp, layers)\n",
    "    \n",
    "    # wandb logs\n",
    "    # wandb.log({\"accuracy\": accuracyTest})\n",
    "    # wandb.log({\"Testing loss\": testLoss})\n",
    "    \n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Test accuracy: \", accuracyTest)\n",
    "    print(\"Test loss: \", testLoss)\n",
    "    \n",
    "    return outputTest\n",
    "\n",
    "# Set hyperparameters and training configuration\n",
    "activation = 'tanh'\n",
    "batchSize = 32\n",
    "epochs = 1\n",
    "hLayers = 3\n",
    "learningRate = 0.1\n",
    "neurons = 128\n",
    "optimizer = 'sgd'\n",
    "\n",
    "# Train the model and get test predictions\n",
    "outputTest = modelTrain(epochs, learningRate, neurons, hLayers, activation, batchSize, optimizer, x_train, y_train, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep ID: zm5ph7l4, Sweep Name: Deep-Learning-Assignment-1\n",
      "Sweep ID: 38qy9h0x, Sweep Name: DL-Assignment-1\n",
      "Sweep ID: xm7jkopc, Sweep Name: DL-Assignment-1_running_Remote\n",
      "Sweep ID: amu7xlj0, Sweep Name: DL-Assignment-1_remote_SqLoss\n",
      "Sweep ID: sluhp8ra, Sweep Name: DL-Assignment-1_sweep2\n",
      "Sweep ID: oc3w4kuz, Sweep Name: DL-Assignment-1_remote_SqLoss_sweep2\n",
      "Sweep ID: 8z40b4y0, Sweep Name: DL-Assignment-1_sqLoss_sweep1\n",
      "Sweep ID: sejxf76k, Sweep Name: DL-Assignment-1_sweep3\n",
      "Sweep ID: nf3mjrmg, Sweep Name: DL-Assignment-1_sweep4\n",
      "Sweep ID: 6xn5laor, Sweep Name: DL-Assignment-1_finalSQloss\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "# List all runs for the project\n",
    "runs = api.runs(\"cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-1-DA6401-DL\")\n",
    "\n",
    "# Collect unique sweep IDs and names\n",
    "sweep_info = {}\n",
    "for run in runs:\n",
    "    if run.sweep:  # Check if the run is part of a sweep\n",
    "        sweep_id = run.sweep.id\n",
    "        if sweep_id not in sweep_info:\n",
    "            sweep_info[sweep_id] = run.sweep.name  # Try getting the sweep name\n",
    "\n",
    "# Print the mapping\n",
    "for sweep_id, sweep_name in sweep_info.items():\n",
    "    print(f\"Sweep ID: {sweep_id}, Sweep Name: {sweep_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
